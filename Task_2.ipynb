{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 10 # Number of topics to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_reader(url):\n",
    "    # Send a GET request to the URL\n",
    "    keyword = 'Claims'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        topic_title = soup.title.string\n",
    "\n",
    "        components = topic_title.split(\" - \")\n",
    "\n",
    "        title = components[1]\n",
    "\n",
    "        type_patent = components[2]\n",
    "        # Exclude content within table elements\n",
    "        for table in soup.find_all('table'):\n",
    "            table.extract()\n",
    "            \n",
    "        # Find all text on the webpage\n",
    "        all_text = soup.get_text()\n",
    "\n",
    "        # Split the text into sentences with periods as separators\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', all_text)\n",
    "\n",
    "        # Initialize a flag to indicate when to start adding sentences\n",
    "        add_sentences = False\n",
    "        assertion_words = [\"claim\", \"assert\", \"argue\", \"propose\", \"conclude\",\n",
    "                        \"suggest\", \"state\", \"believe\", \"affirm\", \"maintain\", \"contend\", \"insist\" , \"apparatus\"]\n",
    "        \n",
    "        # List to store sentences containing the keyword\n",
    "        keyword_sentences = []\n",
    "        # Iterate through the sentences\n",
    "        for sentence in sentences:\n",
    "            # Check if the keyword is found in the sentence\n",
    "            if keyword in sentence:\n",
    "                # Set the flag to True to start adding sentences\n",
    "                add_sentences = True\n",
    "\n",
    "\n",
    "            # Add the sentence if the flag is True and it's not empty\n",
    "            if add_sentences and any(word.lower() in sentence.lower() for word in assertion_words) and sentence.strip().endswith(\".\") and title not in sentence and type_patent not in sentence and \"Fig\" not in sentence and \"Claims (\" not in sentence:\n",
    "                keyword_sentences.append(sentence.strip())\n",
    "\n",
    "        return keyword_sentences\n",
    "    else:\n",
    "        print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to assign topic to a sentence based on occurrence of keywords\n",
    "def assign_topic(sentences, topic_keywords):\n",
    "    assigned = []\n",
    "    remaining_sentences = sentences.copy()  \n",
    "    # Iterate over each sentence\n",
    "    for topic, keywords in topic_keywords.items():\n",
    "        if not remaining_sentences:\n",
    "            break\n",
    "        best_topic = \"Other\"\n",
    "        best_score = 0 # Track the highest score for the assigned topic\n",
    "        score = 0 \n",
    "        # Iterate over each topic and calculate the score for the sentence\n",
    "        for sentence in remaining_sentences:\n",
    "            matches = sum(keyword in sentence.lower() for keyword in keywords)\n",
    "            score = matches / len(keywords)  # Calculate the suitability score\n",
    "            if best_score < score:\n",
    "                best_score = score\n",
    "                best_topic = topic\n",
    "        id = sentences.index(sentence)\n",
    "        assigned.append((best_topic, id))\n",
    "        remaining_sentences.remove(sentence)\n",
    "\n",
    "    return assigned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_render(text):\n",
    "    # Define keywords for each topic\n",
    "    topic_keywords = {\n",
    "        \"Wireless\": [\"wireless\", \"loss\"],\n",
    "        \"Telephone\": [\"telephone\", \"phone\"],\n",
    "        \"Call\": [\"call\", \"indication\" , \"conversation\"],\n",
    "        \"Functionality\": [\"functionality\",\"Efficient\" , \"switching\" , \"quality of service\"],\n",
    "        \"Communication\": [\"communication\", \"source\" , \"network\"],\n",
    "        \"Device\": [\"device\"]\n",
    "    }\n",
    "\n",
    "    # Assign topics to sentences\n",
    "    titles = assign_topic(text , topic_keywords)\n",
    "    \n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train(urls):\n",
    "    train_claim_sentences = []\n",
    "    test_claim_sentences = []\n",
    "\n",
    "    # Data Collection and Preprocessing\n",
    "    for i, url in enumerate(urls):\n",
    "        # Fetch and process the text from the URL\n",
    "        text = url_reader(url)\n",
    "        if i == 2:\n",
    "            test_claim_sentences = text\n",
    "        else:\n",
    "            train_claim_sentences.extend(text)\n",
    "    return train_claim_sentences , test_claim_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://patents.google.com/patent/GB2478972A/en?q=(phone)&oq=phone\",\"https://patents.google.com/patent/US9980046B2/en?oq=US9980046B2\", \"https://patents.google.com/patent/US9634864B2/en?oq=US9634864B2\"]\n",
    "train , test = split_test_train(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_the_results(labels, sentences):\n",
    "    cluster_dict = {}\n",
    "    for cluster_num, sentence in zip(labels, sentences):\n",
    "        if cluster_num not in cluster_dict:\n",
    "            cluster_dict[cluster_num] = [sentence]\n",
    "        else:\n",
    "            cluster_dict[cluster_num].append(sentence)\n",
    "\n",
    "    all_text = []\n",
    "    array_num_of_claims = []\n",
    "    text = \"\"\n",
    "    # Print the cluster dictionary\n",
    "    for cluster_num, cluster_sentences in cluster_dict.items():\n",
    "        number_of_claims = 0\n",
    "        for sentence in cluster_sentences:\n",
    "            number_of_claims += 1\n",
    "            text += \" \" + sentence\n",
    "        all_text.append(text)\n",
    "        array_num_of_claims.append(number_of_claims)\n",
    "        text = \"\"\n",
    "\n",
    "    titles = topic_render(all_text)\n",
    "    for title in titles:\n",
    "        print(f\"title: {title[0]}, numbers of claims: {array_num_of_claims[title[1]-1]}\")\n",
    "    print()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate sentence embeddings using SentenceTransformer\n",
    "def generate_embeddings(sentences, model_name='bert-base-nli-mean-tokens'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test KMeans model on new sentences\n",
    "def test_model_KMeansST(kmeans_model, test_claim_sentences, model_name):\n",
    "    test_embeddings = generate_embeddings(test_claim_sentences, model_name)\n",
    "    predicted_clusters = kmeans_model.predict(test_embeddings)\n",
    "    \n",
    "    # Print the predicted clusters for each test sentence\n",
    "    print(\"Predicted Clusters for New Data:\")\n",
    "    for i, cluster_label in enumerate(predicted_clusters):\n",
    "        print(f\"Document {i + 1}: Cluster = {cluster_label + 1}\")\n",
    "    print_the_results(predicted_clusters, test_claim_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_KMeansST(train_claim_sentences , test_claim_sentences):\n",
    "\n",
    "    model_name = 'bert-base-nli-mean-tokens'\n",
    "\n",
    "    # Generate embeddings for training sentences\n",
    "    train_embeddings = generate_embeddings(train_claim_sentences)\n",
    "\n",
    "    # Topic Modeling (K-means clustering)\n",
    "   \n",
    "    kmeans_model = KMeans(n_clusters=num_clusters, random_state=42, init='k-means++', n_init=10)\n",
    "    kmeans_model.fit(train_embeddings)\n",
    "\n",
    "    # Test the KMeans model on new sentences\n",
    "    test_model_KMeansST(kmeans_model, test_claim_sentences , model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: Wireless, numbers of claims: 3\n",
      "title: Other, numbers of claims: 4\n",
      "title: Call, numbers of claims: 6\n",
      "title: Functionality, numbers of claims: 2\n",
      "title: Communication, numbers of claims: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_KMeansST(train , test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each sentence to a vector representation by averaging word vectors\n",
    "def sentence_to_vector(sentence, model):\n",
    "    words = [word for word in sentence.split() if word in model.wv]\n",
    "    if not words:\n",
    "        return np.zeros(model.vector_size)\n",
    "    vectors = [model.wv[word] for word in words]\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_KMeansW2V(kmeans_model , test_claim_sentences , word2vec_model):\n",
    "    new_data_vectors = np.array([sentence_to_vector(sentence, word2vec_model) for sentence in test_claim_sentences])\n",
    "\n",
    "    if len(new_data_vectors) > 0:\n",
    "        new_data_clusters = kmeans_model.predict(new_data_vectors)\n",
    "        \n",
    "        # Calculate distances to cluster centers for probability-like measure\n",
    "        distances = kmeans_model.transform(new_data_vectors)\n",
    "\n",
    "        # Print the predicted topics for each document\n",
    "        print(\"Predicted Clusters for New Data:\")\n",
    "        for i, cluster_label in enumerate(new_data_clusters):\n",
    "            print(f\"Document {i+1}: Cluster = {cluster_label + 1}\")\n",
    "        print_the_results(new_data_clusters, test_claim_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_KMeansW2V(train_claim_sentences, test_claim_sentences):\n",
    "\n",
    "    # Train Word2Vec model\n",
    "    word2vec_model = Word2Vec(sentences=[sentence.split() for sentence in train_claim_sentences], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    train_vectors = np.array([sentence_to_vector(sentence, word2vec_model) for sentence in train_claim_sentences])\n",
    "\n",
    "    # Topic Modeling (K-means clustering)\n",
    "    kmeans_model = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans_model.fit(train_vectors)\n",
    "\n",
    "    test_model_KMeansW2V(kmeans_model , test_claim_sentences , word2vec_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Clusters for New Data:\n",
      "Document 1: Cluster = 4\n",
      "Document 2: Cluster = 9\n",
      "Document 3: Cluster = 10\n",
      "Document 4: Cluster = 1\n",
      "Document 5: Cluster = 4\n",
      "Document 6: Cluster = 4\n",
      "Document 7: Cluster = 4\n",
      "Document 8: Cluster = 10\n",
      "Document 9: Cluster = 10\n",
      "Document 10: Cluster = 10\n",
      "Document 11: Cluster = 4\n",
      "Document 12: Cluster = 5\n",
      "Document 13: Cluster = 4\n",
      "Document 14: Cluster = 4\n",
      "Document 15: Cluster = 2\n",
      "Document 16: Cluster = 10\n",
      "title: Wireless, numbers of claims: 1\n",
      "title: Other, numbers of claims: 1\n",
      "title: Call, numbers of claims: 5\n",
      "title: Functionality, numbers of claims: 1\n",
      "title: Communication, numbers of claims: 7\n",
      "title: Other, numbers of claims: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_KMeansW2V(train , test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_LDA(test_claim_sentences, vectorizer, lda_model):\n",
    "    # Preprocess the new data\n",
    "    new_data_vectorized = vectorizer.transform(test_claim_sentences)\n",
    "    # Predict cluster for the new data\n",
    "    new_data_cluster_distributions = lda_model.transform(new_data_vectorized)\n",
    "    \n",
    "    predict = 0\n",
    "    cluster = 0\n",
    "    new_data_clusters = []\n",
    "    # Print the predicted topics for each document\n",
    "    print(\"Predicted Topics for New Data:\")\n",
    "    for i, cluster_distribution in enumerate(new_data_cluster_distributions):\n",
    "        for cluster_idx, prob in enumerate(cluster_distribution):\n",
    "            if predict < prob:\n",
    "                predict = prob\n",
    "                cluster = cluster_idx + 1\n",
    "        new_data_clusters.append(cluster)\n",
    "        print(f\"Document {i+1}: Cluster: {cluster}\")\n",
    "        predict = 0\n",
    "    print_the_results(new_data_clusters, test_claim_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LDA(train_claim_sentences , test_claim_sentences):\n",
    "    # Preprocessing\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(train_claim_sentences)\n",
    "\n",
    "    # Topic Modeling (LDA)\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_clusters, random_state=42)\n",
    "    lda_model.fit(X)\n",
    "    \n",
    "    # # Store the sentence index and its corresponding topic\n",
    "    sentence_topics = []\n",
    "    for sentence_idx, (topic_idx, _) in enumerate(zip(lda_model.transform(X).argmax(axis=1), test_claim_sentences)):\n",
    "        sentence_topics.append((sentence_idx + 1, topic_idx + 1))\n",
    "  \n",
    "    test_model_LDA(test_claim_sentences, vectorizer, lda_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Topics for New Data:\n",
      "Document 1: Cluster: 2\n",
      "Document 2: Cluster: 2\n",
      "Document 3: Cluster: 2\n",
      "Document 4: Cluster: 2\n",
      "Document 5: Cluster: 2\n",
      "Document 6: Cluster: 8\n",
      "Document 7: Cluster: 2\n",
      "Document 8: Cluster: 2\n",
      "Document 9: Cluster: 2\n",
      "Document 10: Cluster: 2\n",
      "Document 11: Cluster: 8\n",
      "Document 12: Cluster: 8\n",
      "Document 13: Cluster: 8\n",
      "Document 14: Cluster: 8\n",
      "Document 15: Cluster: 8\n",
      "Document 16: Cluster: 8\n",
      "title: Wireless, numbers of claims: 9\n",
      "title: Other, numbers of claims: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_LDA(train , test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
