{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 10 # Number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_reader(url):\n",
    "        \"\"\"\n",
    "        Scrapes the webpage and extracts sentences containing specified keywords.\n",
    "        \"\"\"\n",
    "        # Keyword to search to start from this line\n",
    "        keyword = 'Claims'\n",
    "\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            # Extract title and type of patent from webpage title\n",
    "            topic_title = soup.title.string\n",
    "            components = topic_title.split(\" - \")\n",
    "            # Patent title\n",
    "            title = components[1]\n",
    "            # Webpage title\n",
    "            type_patent = components[2]\n",
    "\n",
    "            # Exclude content within table elements\n",
    "            for table in soup.find_all('table'):\n",
    "                table.extract()\n",
    "\n",
    "            # Find all text on the webpage\n",
    "            all_text = soup.get_text()\n",
    "\n",
    "            # Split the text into sentences with periods as separators\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', all_text)\n",
    "\n",
    "            # Initialize a flag to indicate when to start adding sentences\n",
    "            add_sentences = False\n",
    "            assertion_words = [\"claim\", \"assert\", \"argue\", \"propose\", \"conclude\",\n",
    "                               \"suggest\", \"state\", \"believe\", \"affirm\", \"maintain\", \"contend\", \"insist\", \"apparatus\"]\n",
    "\n",
    "            # List to store sentences containing the keyword\n",
    "            keyword_sentences = []\n",
    "\n",
    "            # Iterate through the sentences\n",
    "            for sentence in sentences:\n",
    "                # Check if the keyword to start from is found in the sentence\n",
    "                if keyword in sentence:\n",
    "                    # Set the flag to True to start adding sentences\n",
    "                    add_sentences = True\n",
    "\n",
    "                # Add the sentence if the flag is True and it meets criteria\n",
    "                if add_sentences and any(word.lower() in sentence.lower() for word in assertion_words) and \\\n",
    "                        sentence.strip().endswith(\".\") and title not in sentence and type_patent not in sentence \\\n",
    "                        and \"Fig\" not in sentence and \"Claims (\" not in sentence:\n",
    "                    keyword_sentences.append(sentence.strip())\n",
    "\n",
    "            return keyword_sentences\n",
    "        else:\n",
    "            print(\"Failed to fetch the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to assign topic to a sentence based on occurrence of keywords\n",
    "def assign_topic(text, topic_keywords):\n",
    "    assigned = []\n",
    "    # Copy the original text for reuse the text\n",
    "    remaining_text = text.copy()  \n",
    "    # Iterate over each topic and evaluate the best score he have for the specific sentences\n",
    "    for topic, keywords in topic_keywords.items():\n",
    "        # If there is no more sentences in the text\n",
    "        if not remaining_text:\n",
    "            break\n",
    "        best_topic = \"Other\"\n",
    "        best_score = 0 # Track the highest score for the assigned topic\n",
    "        score = 0 \n",
    "        # Iterate over each sentence and calculate the score for the sentence\n",
    "        for sentence in remaining_text:\n",
    "            matches = sum(keyword in sentence.lower() for keyword in keywords)\n",
    "            # Calculate the suitability score\n",
    "            score = matches / len(keywords)  \n",
    "            # Check the high score and adapt the topic\n",
    "            if best_score < score:\n",
    "                best_score = score\n",
    "                best_topic = topic\n",
    "        # Get the original index from the text\n",
    "        id = text.index(sentence)\n",
    "        # Assigned the topic to the right sentence\n",
    "        assigned.append((best_topic, id))\n",
    "        # Remove the sentence to iterate over the remaining sentences.\n",
    "        remaining_text.remove(sentence)\n",
    "\n",
    "    return assigned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_render(text):\n",
    "    # Define keywords for each title\n",
    "    topic_keywords = {\n",
    "        \"Wireless\": [\"wireless\", \"loss\"],\n",
    "        \"Telephone\": [\"telephone\", \"phone\"],\n",
    "        \"Call\": [\"call\", \"indication\" , \"conversation\" , \"second party\"],\n",
    "        \"Functionality\": [\"functionality\",\"Efficient\" , \"switching\" , \"quality of service\"],\n",
    "        \"Communication\": [\"communication\", \"source\" , \"network\" , \"signal\"],\n",
    "        \"Device\": [\"device\" ,\"tool\",\"machine\"],\n",
    "        \"Service\": [\"service\"],\n",
    "        \"Protocols\": [\"protocols\"],\n",
    "        \"Microphones\": [\"microphones\", \"speaker\" ,\"sound Sensors\" ,\"acoustic Sensors\" , \"sound output\"],\n",
    "    }\n",
    "\n",
    "\n",
    "    # Assign title to sentences\n",
    "    titles = assign_topic(text , topic_keywords)\n",
    "    \n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_claims(urls):\n",
    "\n",
    "    claim_sentences = []\n",
    "\n",
    "    # Data Collection and Preprocessing\n",
    "    for url in urls:\n",
    "        # Fetch and process the text from the URL\n",
    "        text = url_reader(url)\n",
    "        # Aggregate all the data together\n",
    "        claim_sentences.extend(text)\n",
    "    return claim_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of URLs provided in the homework\n",
    "urls = [\"https://patents.google.com/patent/GB2478972A/en?q=(phone)&oq=phone\",\"https://patents.google.com/patent/US9980046B2/en?oq=US9980046B2\", \"https://patents.google.com/patent/US9634864B2/en?oq=US9634864B2\"]\n",
    "test = extract_test_claims(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_the_results(labels, sentences):\n",
    "    \"\"\"\n",
    "    Prints the results of clustering sentences.\n",
    "\n",
    "    :param labels: List of cluster labels assigned to each sentence\n",
    "    :param sentences: List of sentences to be clustered\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to store sentences for each cluster\n",
    "    cluster_dict = {}\n",
    "\n",
    "    # Filling up the cluster dictionary with data. \n",
    "    for cluster_num, sentence in zip(labels, sentences):\n",
    "        if cluster_num not in cluster_dict:\n",
    "            cluster_dict[cluster_num] = [sentence]\n",
    "        else:\n",
    "            cluster_dict[cluster_num].append(sentence)\n",
    "\n",
    "    # Initialize lists to store aggregated text and number of claims per cluster\n",
    "    all_text = []\n",
    "    array_num_of_claims = []\n",
    "    text = \"\"\n",
    "\n",
    "    # Aggregate text for each cluster and count the number of claims\n",
    "    for cluster_num, cluster_sentences in cluster_dict.items():\n",
    "        number_of_claims = 0\n",
    "        for sentence in cluster_sentences:\n",
    "            number_of_claims += 1\n",
    "            # Group all the sentences from the same cluster\n",
    "            text += \" \" + sentence\n",
    "        all_text.append(text)\n",
    "        array_num_of_claims.append(number_of_claims)\n",
    "        text = \"\"\n",
    "\n",
    "    # Get titles for each aggregated text\n",
    "    titles = topic_render(all_text)\n",
    "\n",
    "    agg_title = {}\n",
    "\n",
    "    # Update the aggregated titles with their numbers of claims from the array_num_of_claims extracting by index\n",
    "    for title in titles:\n",
    "        # Check if the title is already in the aggregated titles dictionary\n",
    "        if title[0] in agg_title:\n",
    "            # If yes, add the number of claims to the existing count\n",
    "            agg_title[title[0]] += array_num_of_claims[title[1]-1]\n",
    "        else:\n",
    "            # If no, initialize the count with the number of claims\n",
    "            agg_title[title[0]] = array_num_of_claims[title[1]-1]\n",
    "\n",
    "    # Print the aggregated titles and their numbers of claims\n",
    "    for title, claims in agg_title.items():\n",
    "        print(f\"title: {title}, numbers of claims: {claims}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(sentences, model_name='bert-base-nli-mean-tokens'):\n",
    "    \"\"\"\n",
    "    Generates sentence embeddings using SentenceTransformer model.\n",
    "\n",
    "    :param sentences: List of sentences to generate embeddings for\n",
    "    :param model_name: Name of the SentenceTransformer model to use (default: 'bert-base-nli-mean-tokens')\n",
    "    :return: List of sentence embeddings\n",
    "    \"\"\"\n",
    "    # Initialize SentenceTransformer model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings for the sentences\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_KMeansST(test_claim_sentences):\n",
    "    \"\"\"\n",
    "    Performs K-means clustering on the test claim sentences using SentenceTransformer embeddings.\n",
    "\n",
    "    :param test_claim_sentences: List of test claim sentences\n",
    "    \"\"\"\n",
    "    # Define the SentenceTransformer model to use\n",
    "    model_name = 'bert-base-nli-mean-tokens'\n",
    "\n",
    "    # Initialize KMeans model for clustering\n",
    "    kmeans_model = KMeans(\n",
    "        n_clusters=num_clusters,   # Number of clusters to form\n",
    "                                   # Defualt parameters:\n",
    "        random_state=42,           # Seed for random number generation\n",
    "        init='k-means++',          # Method for initializing centroids \n",
    "        n_init=10                  # Number of times the algorithm will be run with different centroid seeds\n",
    "    )\n",
    "    \n",
    "    # Generate sentence embeddings for the test claim sentences using SentenceTransformer\n",
    "    test_embeddings = generate_embeddings(test_claim_sentences, model_name)\n",
    "    \n",
    "    # Fit KMeans model on the generated embeddings\n",
    "    kmeans_model.fit(test_embeddings)\n",
    "    \n",
    "    # Predict clusters for the test claim sentences\n",
    "    predicted_clusters = kmeans_model.predict(test_embeddings)\n",
    "    \n",
    "    # Print the predicted clusters for each test sentence\n",
    "    print(\"Predicted Clusters for New Data:\")\n",
    "    for i, cluster_label in enumerate(predicted_clusters):\n",
    "        print(f\"Document {i + 1}: Cluster = {cluster_label + 1}\")\n",
    "    \n",
    "    # Print the results of clustering\n",
    "    print_the_results(predicted_clusters, test_claim_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Clusters for New Data:\n",
      "Document 1: Cluster = 8\n",
      "Document 2: Cluster = 5\n",
      "Document 3: Cluster = 8\n",
      "Document 4: Cluster = 1\n",
      "Document 5: Cluster = 1\n",
      "Document 6: Cluster = 10\n",
      "Document 7: Cluster = 10\n",
      "Document 8: Cluster = 9\n",
      "Document 9: Cluster = 9\n",
      "Document 10: Cluster = 9\n",
      "Document 11: Cluster = 1\n",
      "Document 12: Cluster = 7\n",
      "Document 13: Cluster = 8\n",
      "Document 14: Cluster = 2\n",
      "Document 15: Cluster = 2\n",
      "Document 16: Cluster = 2\n",
      "Document 17: Cluster = 2\n",
      "Document 18: Cluster = 5\n",
      "Document 19: Cluster = 5\n",
      "Document 20: Cluster = 5\n",
      "Document 21: Cluster = 2\n",
      "Document 22: Cluster = 7\n",
      "Document 23: Cluster = 2\n",
      "Document 24: Cluster = 5\n",
      "Document 25: Cluster = 5\n",
      "Document 26: Cluster = 2\n",
      "Document 27: Cluster = 2\n",
      "Document 28: Cluster = 2\n",
      "Document 29: Cluster = 2\n",
      "Document 30: Cluster = 2\n",
      "Document 31: Cluster = 3\n",
      "Document 32: Cluster = 6\n",
      "Document 33: Cluster = 1\n",
      "Document 34: Cluster = 1\n",
      "Document 35: Cluster = 6\n",
      "Document 36: Cluster = 3\n",
      "Document 37: Cluster = 6\n",
      "Document 38: Cluster = 4\n",
      "Document 39: Cluster = 4\n",
      "Document 40: Cluster = 4\n",
      "Document 41: Cluster = 3\n",
      "Document 42: Cluster = 6\n",
      "Document 43: Cluster = 3\n",
      "Document 44: Cluster = 4\n",
      "Document 45: Cluster = 4\n",
      "Document 46: Cluster = 4\n",
      "title: Wireless, numbers of claims: 4\n",
      "title: Telephone, numbers of claims: 4\n",
      "title: Call, numbers of claims: 11\n",
      "title: Other, numbers of claims: 18\n",
      "title: Communication, numbers of claims: 3\n"
     ]
    }
   ],
   "source": [
    "model_KMeansST(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(sentence, model):\n",
    "    \"\"\"\n",
    "    Convert each sentence to a vector representation by averaging word vectors.\n",
    "\n",
    "    :param sentence: Input sentence to convert\n",
    "    :param model: Word2Vec model used for word embeddings\n",
    "    :return: Vector representation of the sentence\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence and filter out words not present in the Word2Vec model\n",
    "    words = [word for word in sentence.split() if word in model.wv]\n",
    "    \n",
    "    # If no words are found in the model, return a zero vector\n",
    "    if not words:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    # Get word vectors for the words present in the model\n",
    "    vectors = [model.wv[word] for word in words]\n",
    "    \n",
    "    # Calculate the mean of word vectors to obtain the sentence vector\n",
    "    return np.mean(vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_KMeansW2V(test_claim_sentences):\n",
    "    \"\"\"\n",
    "    Performs K-means clustering on the Word2Vec vectors of test claim sentences.\n",
    "\n",
    "    :param test_claim_sentences: List of test claim sentences\n",
    "    \"\"\"\n",
    "    # Word2Vec model\n",
    "    word2vec_model = Word2Vec(\n",
    "        sentences=[sentence.split() for sentence in test_claim_sentences],  # Convert sentences to word lists\n",
    "        vector_size=100,  # Dimensionality of word vectors\n",
    "        window=5,         # Maximum distance between the current and predicted word within a sentence\n",
    "        min_count=1,      # Ignore words with a frequency lower than this\n",
    "        workers=4         # Number of threads to train the model\n",
    "    )\n",
    "\n",
    "    # Generate Word2Vec vectors for test claim sentences\n",
    "    test_vectors = np.array([sentence_to_vector(sentence, word2vec_model) for sentence in test_claim_sentences])\n",
    "\n",
    "    # Cluster Modeling (K-means clustering)\n",
    "    kmeans_model = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    \n",
    "    # Fit KMeans model on the Word2Vec vectors\n",
    "    kmeans_model.fit(test_vectors)\n",
    "\n",
    "    # Predict clusters for the test claim sentences\n",
    "    if len(test_vectors) > 0:\n",
    "        new_data_clusters = kmeans_model.predict(test_vectors)\n",
    "\n",
    "        # Print the predicted clusters for each document\n",
    "        print(\"Predicted Clusters for New Data:\")\n",
    "        for i, cluster_label in enumerate(new_data_clusters):\n",
    "            print(f\"Document {i+1}: Cluster = {cluster_label + 1}\")\n",
    "        # Print the results of clustering\n",
    "        print_the_results(new_data_clusters, test_claim_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Clusters for New Data:\n",
      "Document 1: Cluster = 9\n",
      "Document 2: Cluster = 9\n",
      "Document 3: Cluster = 9\n",
      "Document 4: Cluster = 5\n",
      "Document 5: Cluster = 8\n",
      "Document 6: Cluster = 5\n",
      "Document 7: Cluster = 5\n",
      "Document 8: Cluster = 8\n",
      "Document 9: Cluster = 8\n",
      "Document 10: Cluster = 9\n",
      "Document 11: Cluster = 5\n",
      "Document 12: Cluster = 5\n",
      "Document 13: Cluster = 9\n",
      "Document 14: Cluster = 6\n",
      "Document 15: Cluster = 3\n",
      "Document 16: Cluster = 7\n",
      "Document 17: Cluster = 3\n",
      "Document 18: Cluster = 1\n",
      "Document 19: Cluster = 6\n",
      "Document 20: Cluster = 6\n",
      "Document 21: Cluster = 1\n",
      "Document 22: Cluster = 9\n",
      "Document 23: Cluster = 7\n",
      "Document 24: Cluster = 6\n",
      "Document 25: Cluster = 6\n",
      "Document 26: Cluster = 3\n",
      "Document 27: Cluster = 7\n",
      "Document 28: Cluster = 6\n",
      "Document 29: Cluster = 6\n",
      "Document 30: Cluster = 3\n",
      "Document 31: Cluster = 4\n",
      "Document 32: Cluster = 4\n",
      "Document 33: Cluster = 4\n",
      "Document 34: Cluster = 4\n",
      "Document 35: Cluster = 4\n",
      "Document 36: Cluster = 10\n",
      "Document 37: Cluster = 4\n",
      "Document 38: Cluster = 2\n",
      "Document 39: Cluster = 2\n",
      "Document 40: Cluster = 2\n",
      "Document 41: Cluster = 10\n",
      "Document 42: Cluster = 10\n",
      "Document 43: Cluster = 10\n",
      "Document 44: Cluster = 2\n",
      "Document 45: Cluster = 2\n",
      "Document 46: Cluster = 2\n",
      "title: Wireless, numbers of claims: 4\n",
      "title: Telephone, numbers of claims: 6\n",
      "title: Call, numbers of claims: 2\n",
      "title: Other, numbers of claims: 24\n",
      "title: Communication, numbers of claims: 4\n"
     ]
    }
   ],
   "source": [
    "model_KMeansW2V(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LDA(test_claim_sentences):\n",
    "    \"\"\"\n",
    "    Performs Latent Dirichlet Allocation (LDA) topic modeling on the test claim sentences.\n",
    "\n",
    "    :param test_claim_sentences: List of test claim sentences\n",
    "    \"\"\"\n",
    "    # Preprocessing: Convert text into a numerical format\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(test_claim_sentences)\n",
    "\n",
    "    # Topic Modeling (LDA)\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_clusters, random_state=42)\n",
    "    lda_model.fit(X)\n",
    "\n",
    "    # Preprocess the new data\n",
    "    test_vectorized = vectorizer.transform(test_claim_sentences)\n",
    "    \n",
    "    # Predict cluster for the new data\n",
    "    test_cluster_distributions = lda_model.transform(test_vectorized)\n",
    "    \n",
    "    # Initialize variables for predicting and storing clusters\n",
    "    predict = 0\n",
    "    cluster = 0\n",
    "    test_clusters = []\n",
    "    \n",
    "    # Print the predicted topics for each document\n",
    "    print(\"Predicted Topics for New Data:\")\n",
    "    for i, cluster_distribution in enumerate(test_cluster_distributions):\n",
    "        for cluster_idx, prob in enumerate(cluster_distribution):\n",
    "            # Find the cluster with the highest probability\n",
    "            if predict < prob:\n",
    "                predict = prob\n",
    "                cluster = cluster_idx + 1\n",
    "        test_clusters.append(cluster)\n",
    "        print(f\"Document {i+1}: Cluster: {cluster}\")\n",
    "        predict = 0\n",
    "    \n",
    "    # Display the results of clustering\n",
    "    print_the_results(test_clusters, test_claim_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Topics for New Data:\n",
      "Document 1: Cluster: 9\n",
      "Document 2: Cluster: 9\n",
      "Document 3: Cluster: 3\n",
      "Document 4: Cluster: 3\n",
      "Document 5: Cluster: 9\n",
      "Document 6: Cluster: 9\n",
      "Document 7: Cluster: 10\n",
      "Document 8: Cluster: 9\n",
      "Document 9: Cluster: 9\n",
      "Document 10: Cluster: 9\n",
      "Document 11: Cluster: 3\n",
      "Document 12: Cluster: 3\n",
      "Document 13: Cluster: 9\n",
      "Document 14: Cluster: 8\n",
      "Document 15: Cluster: 2\n",
      "Document 16: Cluster: 1\n",
      "Document 17: Cluster: 1\n",
      "Document 18: Cluster: 3\n",
      "Document 19: Cluster: 5\n",
      "Document 20: Cluster: 5\n",
      "Document 21: Cluster: 2\n",
      "Document 22: Cluster: 2\n",
      "Document 23: Cluster: 1\n",
      "Document 24: Cluster: 5\n",
      "Document 25: Cluster: 5\n",
      "Document 26: Cluster: 1\n",
      "Document 27: Cluster: 1\n",
      "Document 28: Cluster: 8\n",
      "Document 29: Cluster: 2\n",
      "Document 30: Cluster: 1\n",
      "Document 31: Cluster: 3\n",
      "Document 32: Cluster: 7\n",
      "Document 33: Cluster: 7\n",
      "Document 34: Cluster: 7\n",
      "Document 35: Cluster: 7\n",
      "Document 36: Cluster: 7\n",
      "Document 37: Cluster: 7\n",
      "Document 38: Cluster: 5\n",
      "Document 39: Cluster: 5\n",
      "Document 40: Cluster: 5\n",
      "Document 41: Cluster: 3\n",
      "Document 42: Cluster: 7\n",
      "Document 43: Cluster: 3\n",
      "Document 44: Cluster: 5\n",
      "Document 45: Cluster: 5\n",
      "Document 46: Cluster: 5\n",
      "title: Wireless, numbers of claims: 10\n",
      "title: Telephone, numbers of claims: 6\n",
      "title: Call, numbers of claims: 4\n",
      "title: Functionality, numbers of claims: 2\n",
      "title: Communication, numbers of claims: 1\n",
      "title: Other, numbers of claims: 23\n"
     ]
    }
   ],
   "source": [
    "model_LDA(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
